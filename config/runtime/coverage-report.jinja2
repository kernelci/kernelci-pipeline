{# -*- mode: Python -*- -#}
{# SPDX-License-Identifier: LGPL-2.1-or-later -#}

{%- extends 'base/python.jinja2' %}

{%- block python_imports %}
{{ super() }}
import gzip
import json
import shutil
import subprocess
{%- endblock %}

{%- block python_local_imports %}
{{ super() }}
import kernelci.api.helper
{%- endblock %}

{%- block python_globals %}
{{ super() }}
DEFAULT_FAIL_RESULT = {
    'node': {
        'result': 'fail',
        'artifacts': {},
    },
    'child_nodes': [],
}
DEFAULT_PASS_RESULT = {
    'node': {
        'result': 'pass',
        'artifacts': {},
    },
    'child_nodes': [],
}
{% endblock %}

{% block python_job -%}
class Job(BaseJob):
    def _upload_artifacts(self):
        artifacts = {}
        storage = self._get_storage()
        if storage and self._node:
            root_path = '-'.join([JOB_NAME, self._node['id']])
            print(f"Uploading artifacts to {root_path}")
            for name, file_path in self._artifacts.items():
                if os.path.exists(file_path):
                    file_url = storage.upload_single(
                        (file_path, os.path.basename(file_path)), root_path
                    )
                    print(file_url)
                    artifacts[name] = file_url
        return artifacts

    def _extract_coverage(self, summary_file, node=None):
        if node is None:
            node = self._node

        child_nodes = []

        with open(summary_file, encoding='utf-8') as summary_json:
            summary = json.load(summary_json)
            node_data = node['data']

            func_data = node_data.copy()
            func_percent = summary.get('function_percent')
            if func_percent is not None:
                func_data['misc'] = {'measurement': func_percent}
                child_nodes += [
                    {
                        'node': {
                            'kind': 'test',
                            'name': 'coverage.functions',
                            'result': 'pass',
                            'state': 'done',
                            'data': func_data,
                        },
                        'child_nodes': [],
                    },
                ]

            line_data = node_data.copy()
            line_percent = summary.get('line_percent')
            if line_percent is not None:
                line_data['misc'] = {'measurement': line_percent}
                child_nodes += [
                    {
                        'node': {
                            'kind': 'test',
                            'name': 'coverage.lines',
                            'result': 'pass',
                            'state': 'done',
                            'data': line_data,
                        },
                        'child_nodes': [],
                    },
                ]

        return {
            'node': {
                'result': 'pass' if node['id'] == self._node['id'] else node['result'],
                'artifacts': {},
            },
            'child_nodes': child_nodes,
        }

    def _process_coverage_data(self, node):
        coverage_dir = os.path.join(self._workspace, f"coverage-{node['id']}")

        try:
            data_url = self._get_artifact_url(node, 'coverage_data')
        except:
            self._logfile.write(f"WARNING: No coverage data available for {node['id']}\n")
            # Return a "pass" result as this isn't an error, we can carry on
            return None, DEFAULT_PASS_RESULT

        try:
            self._get_source(data_url, path=coverage_dir)
            self._logfile.write(f"Downloaded coverage data from {data_url}\n")
        except:
            self._logfile.write(f"ERROR: Unable to download coverage data for {node['id']}\n")
            return None, DEFAULT_FAIL_RESULT

        # We now have raw coverage data available, process it
        tracefile = coverage_dir + '.json'
        json_summary = coverage_dir + '.summary.json'
        self._logfile.write(f"--- Processing coverage data for {node['id']} ---\n")
        cmd = subprocess.run(self._gcovr_cmd + [
            '--object-directory', coverage_dir,
            '--json', tracefile,
            '--json-summary', json_summary,
        ], stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)
        self._logfile.write(cmd.stdout)

        try:
            cmd.check_returncode()
        except:
            self._logfile.write(f"ERROR: Unable to process coverage data for {node['id']}")
            return None, DEFAULT_FAIL_RESULT

        results = self._extract_coverage(json_summary, node=node)
        return tracefile, results


    def _retrieve_tracefile(self, node):
        process_nodes = self._api.node.findfast({'parent': node['id'], 'kind': 'process'})
        for pnode in process_nodes:
            if not pnode['name'].startswith('coverage-report'):
                self._logfile.write(f"Ignoring post-process node {pnode['id']} (pnode['name'])\n")
                continue

            try:
                data_url = self._get_artifact_url(pnode, 'tracefile')
            except:
                self._logfile.write(f"WARNING: No tracefile artifact in {pnode['id']}\n")
                continue

            resp = requests.get(data_url, stream=True)
            resp.raise_for_status()
            self._logfile.write(f"Downloaded tracefile from {data_url}\n")
            trace_name = os.path.basename(urllib.parse.urlparse(data_url).path)
            tracefile = os.path.join(self._workspace, trace_name.removesuffix('.gz'))
            with gzip.open(resp.raw, mode='rb') as f_in:
                with open(tracefile, 'wb') as f_out:
                    shutil.copyfileobj(f_in, f_out)
            self._logfile.write(f"Uncompressed tracefile into {tracefile}\n")

            return tracefile

        return None

    def _run(self, src_path):
        self._artifacts = {}
        api_helper = kernelci.api.helper.APIHelper(self._api)
        parent_node = self._api.node.get(self._node['parent'])

        log_path = os.path.join(self._workspace, f"log.txt")
        self._logfile = open(log_path, mode='w')

        self._logfile.write("Getting coverage source...\n")
        tarball_url = self._get_artifact_url(self._node, 'coverage_source_tar_xz')
        self._get_source(tarball_url)
        # Not getting src_path from _get_source() as it doesn't work in our case
        # We do know that the top-level folder is named 'linux' however, so let's
        # just use that
        self._src_path = os.path.join(self._workspace, 'linux')
        self._logfile.write(f"Coverage source downloaded from {tarball_url}\n")

        self._gcovr_cmd = [
            'gcovr',
            '--gcov-ignore-parse-errors',
            '--root', self._src_path,
        ]

        output_base = os.path.join(self._workspace, f"coverage-{self._node['parent']}")

        if parent_node['kind'] == 'job':
            (tracefile, results) = self._process_coverage_data(parent_node)
            if tracefile is None:
                self._logfile.close()
                self._artifacts = {'log': log_path}
                return results

        else:
            tracefiles = []
            child_nodes = self._api.node.findfast({'parent': parent_node['id'], 'kind': 'job'})
            # Download and process coverage data for all child nodes
            for cnode in child_nodes:
                if cnode['id'] == self._node['id']:
                    self._logfile.write(f"Skipping self ({cnode['id']})\n")
                    continue

                try:
                    job_trace = self._retrieve_tracefile(cnode)
                except:
                    self._logfile.write(f"WARNING: No tracefile available for {cnode['id']}\n")
                    continue

                if job_trace:
                    tracefiles += [job_trace]

            if len(tracefiles) == 0:
                self._logfile.write(f"WARNING: No tracefile found in children for {parent_node['id']}\n")
                self._logfile.close()
                self._artifacts = {'log': log_path}
                return DEFAULT_PASS_RESULT

            # Coverage data has been processed for all job nodes, we can now merge the tracefiles
            tracefile = output_base + '.json'
            json_summary = output_base + '.summary.json'
            args = self._gcovr_cmd
            for trace in tracefiles:
                args += ['--add-tracefile', trace]
            args += [
                '--json', tracefile,
                '--json-summary', json_summary,
            ]

            self._logfile.write("--- Merging tracefiles ---\n")
            cmd = subprocess.run(args,
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                text=True)
            self._logfile.write(cmd.stdout)

            results = self._extract_coverage(json_summary, node=parent_node)

        self._logfile.write("--- Generating HTML report ---\n")
        html_report = output_base + '.html'
        args = self._gcovr_cmd + [
            '--add-tracefile', tracefile,
            '--html', html_report,
        ]
        if parent_node['kind'] == 'kbuild':
            lcov_tracefile = output_base + '.info'
            args += ['--lcov', lcov_tracefile]

        cmd = subprocess.run(args,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            text=True)
        self._logfile.write(cmd.stdout)

        # Ensure job completed successfully or report failure
        try:
            cmd.check_returncode()
        except:
            self._logfile.write(f"ERROR: Unable to generate coverage report\n")
            self._logfile.close()
            self._artifacts = {'log': log_path}
            return DEFAULT_FAIL_RESULT

        self._logfile.write("--- Compressing artifacts ---\n")
        compressed_trace = tracefile + '.gz'
        with open(tracefile, 'rb') as f_in:
            with gzip.open(compressed_trace, 'wb') as f_out:
                shutil.copyfileobj(f_in, f_out)

        # Finish writing the job log and upload it along with other artifacts
        self._logfile.write("--- Job successful ---\n")
        self._logfile.close()

        self._artifacts = {
            'coverage_report': html_report,
            'tracefile': compressed_trace,
            'log': log_path,
        }

        if parent_node['kind'] == 'kbuild':
            compressed_lcov = lcov_tracefile + '.gz'
            with open(lcov_tracefile, 'rb') as f_in:
                with gzip.open(compressed_lcov, 'wb') as f_out:
                    shutil.copyfileobj(f_in, f_out)
            self._artifacts['lcov_tracefile'] = compressed_lcov

        return results

    def _submit(self, result):
        # Ensure top-level name is kept the same
        result = result.copy()
        # Update node from API, as we might have new fields
        # such as k8s_context
        node_id = self._node['id']
        self._node = self._api.node.get(node_id)
        # Upload artifacts and update node accordingly
        artifacts = self._upload_artifacts()
        result['node']['name'] = self._node['name']
        result['node']['state'] = 'done'
        result['node']['artifacts'] = artifacts
        # Actually submit the results
        api_helper = kernelci.api.helper.APIHelper(self._api)
        api_helper.submit_results(result, self._node)

{% endblock %}
