{# -*- mode: Python -*- -#}
{# SPDX-License-Identifier: LGPL-2.1-or-later -#}

{%- extends 'base/python.jinja2' %}

{%- block python_imports %}
{{ super() }}
import gzip
import json
import re
import shutil
import subprocess
{%- endblock %}

{%- block python_local_imports %}
{{ super() }}
import kernelci.api.helper
from kernelci import shell_cmd
{%- endblock %}

{%- block python_globals %}
{{ super() }}
{% endblock %}

{% block python_job -%}
class Job(BaseJob):
    def _upload_artifacts(self):
        artifacts = {}
        storage = self._get_storage()
        if storage and self._node:
            root_path = '-'.join([JOB_NAME, self._node['id']])
            print(f"Uploading artifacts to {root_path}")
            for name, file_path in self._artifacts.items():
                if os.path.exists(file_path):
                    file_url = storage.upload_single(
                        (file_path, os.path.basename(file_path)), root_path
                    )
                    print(file_url)
                    artifacts[name] = file_url
        return artifacts

    def _extract_coverage(self, summary, node=None):
        if node is None:
            node = self._node

        child_nodes = []

        m = re.search(r'lines.*: ([\d\.]*)%.*functions.*: ([\d\.]*)%', summary, flags=re.DOTALL)
        if m:
            node_data = node['data']
            (line_percent, func_percent) = m.groups()

            try:
                line_data = node_data.copy()
                line_data['misc'] = {'measurement': float(line_percent)}
                child_nodes += [
                    {
                        'node': {
                            'kind': 'test',
                            'name': 'coverage.lines',
                            'result': 'pass',
                            'state': 'done',
                            'data': line_data,
                        },
                        'child_nodes': [],
                    },
                ]
            except ValueError:
                print(f"'{line_percent}' is not a floating-point value!")

            try:
                func_data = node_data.copy()
                func_data['misc'] = {'measurement': float(func_percent)}
                child_nodes += [
                    {
                        'node': {
                            'kind': 'test',
                            'name': 'coverage.functions',
                            'result': 'pass',
                            'state': 'done',
                            'data': func_data,
                        },
                        'child_nodes': [],
                    },
                ]
            except ValueError:
                print(f"'{func_percent}' is not a floating-point value!")

        return {
            'node': {
                'result': 'pass' if node['id'] == self._node['id'] else node['result'],
                'artifacts': {},
            },
            'child_nodes': child_nodes,
        }

    def _run(self, src_path):
        self._artifacts = {}
        api_helper = kernelci.api.helper.APIHelper(self._api)
        child_nodes = self._api.node.findfast({'parent': self._node['parent']})

        log_path = os.path.join(self._workspace, f"log.txt")
        log_file = open(log_path, mode='w')

        log_file.write("Getting coverage source...\n")
        tarball_url = self._get_artifact_url(self._node, 'coverage_source_tar_xz')
        self._get_source(tarball_url)
        # Not getting src_path from _get_source() as it doesn't work in our case
        # We do know that the top-level folder is named 'linux' however, so let's
        # just use that
        src_path = os.path.join(self._workspace, 'linux')
        log_file.write(f"Coverage source downloaded from {tarball_url}\n")
        nproc = shell_cmd("nproc")
        log_file.write(f"Calling lcov with {nproc} threads\n")

        base_cmd = [
            'lcov',
            '--parallel', nproc,
            '--ignore-errors', 'inconsistent,inconsistent,negative,negative',
        ]
        tracefiles = []

        # Download and process coverage data for all child nodes
        for cnode in child_nodes:
            if cnode['id'] == self._node['id']:
                log_file.write(f"Skipping self ({cnode['id']})\n")
                continue

            coverage_dir = os.path.join(self._workspace, f"coverage-{cnode['id']}")
            try:
                data_url = self._get_artifact_url(cnode, 'coverage_data')
                tracefile = coverage_dir + '.info'
                self._get_source(data_url, path=coverage_dir)
                log_file.write(f"Downloaded coverage data from {data_url}\n")
            except:
                log_file.write(f"WARNING: Unable to download coverage data for {cnode['id']}\n")
                continue

            # We now have raw coverage data available, process it
            log_file.write(f"--- Processing coverage data for {cnode['id']} ---\n")
            cmd = subprocess.run(base_cmd + [
                '--capture',
                '--base-directory', src_path,
                '--directory', coverage_dir,
                '--output-file', tracefile,
            ], stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)
            log_file.write(cmd.stdout)

            try:
                cmd.check_returncode()
            except:
                log_file.write(f"WARNING: Unable to process coverage data for {cnode['id']}")
                continue

            tracefiles += [tracefile]
            results = self._extract_coverage(cmd.stdout, node=cnode)
            # We only want to create child nodes reporting coverage percentages, not actually
            # update the test node
            if len(results['child_nodes']) > 0:
                api_helper.submit_results(results, cnode)

        # Coverage data has been processed for all child nodes, we can now merge the tracefiles
        args = base_cmd
        for trace in tracefiles:
            args += ['--add-tracefile', trace]

        output_base = os.path.join(self._workspace, f"coverage-{self._node['parent']}")
        html_report = os.path.join(self._workspace, f"html-{self._node['parent']}")
        tracefile = output_base + '.info'
        args += ['--output-file', tracefile]

        log_file.write("--- Merging tracefiles ---\n")
        cmd = subprocess.run(args,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            text=True)
        log_file.write(cmd.stdout)

        log_file.write("--- Generating HTML report ---\n")
        args = [
            'genhtml',
            '--flat',
            '--no-sourceview',
            '--no-sort',
            '--output-directory', html_report,
            tracefile,
        ]
        cmd = subprocess.run(args,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            text=True)
        log_file.write(cmd.stdout)

        # Ensure job completed successfully or report failure
        try:
            cmd.check_returncode()
        except:
            log_file.write(f"ERROR: Unable to generate coverage report\n")
            log_file.close()

            self._artifacts = {'log': log_path}
            return {
                'node': {
                    'result': 'fail',
                    'artifacts': {},
                },
                'child_nodes': [],
            }

        log_file.write("--- Compressing artifacts ---\n")
        compressed_trace = tracefile + '.gz'
        with open(tracefile, 'rb') as f_in:
            with gzip.open(compressed_trace, 'wb') as f_out:
                shutil.copyfileobj(f_in, f_out)

        html_tarball = html_report + '.tar.gz'
        # Delete existing tarball if any
        if os.path.isfile(html_tarball):
            os.unlink(html_tarball)
        with tarfile.open(html_tarball, 'x:gz') as tarball:
            for entry in os.scandir(html_report):
                if entry.is_file():
                    tar_path = entry.path.removeprefix(self._workspace)
                    tarball.add(entry.path, arcname=tar_path)

        # Finish writing the job log and upload it along with other artifacts
        log_file.write("--- Job successful ---\n")
        log_file.close()

        self._artifacts = {
            'coverage_report': html_tarball,
            'tracefile': compressed_trace,
            'log': log_path,
        }

        return self._extract_coverage(cmd.stdout)

        return results

    def _submit(self, result):
        # Ensure top-level name is kept the same
        result = result.copy()
        # Update node from API, as we might have new fields
        # such as k8s_context
        node_id = self._node['id']
        self._node = self._api.node.get(node_id)
        # Upload artifacts and update node accordingly
        artifacts = self._upload_artifacts()
        result['node']['name'] = self._node['name']
        result['node']['state'] = 'done'
        result['node']['artifacts'] = artifacts
        # Actually submit the results
        api_helper = kernelci.api.helper.APIHelper(self._api)
        api_helper.submit_results(result, self._node)

{% endblock %}
